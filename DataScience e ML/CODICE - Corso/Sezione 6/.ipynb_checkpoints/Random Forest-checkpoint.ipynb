{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=1, n_jobs=2)\n",
    "forest.fit(X_train, y_train)\n",
    "plot_decision_regions(X_combined, y_combined, classifier=forest, test_idx=range(105,150))\n",
    "plt.xlabel('petal length')\n",
    "plt.ylabel('petal width')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The parameters in RandomForestRegressor have their significance. The n_jobs is used to specify the\n",
    "parallelization of the computing and signifies the number of jobs running parallel for both fit and predict. The\n",
    "oob_score is a binary variable. Setting it to True means that the model has done an out-of-the-box\n",
    "sampling to make the predictions. The n_estimators specifies the number of trees our random forest will\n",
    "have. It has been chosen to be 10 just for illustrative purposes. One can try a higher number and see whether\n",
    "it improves the error rate or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf.oob_prediction_\n",
    "Let us now make the predictions a part of the data frame and have a look at it.\n",
    "data['rf_pred']=rf.oob_prediction_\n",
    "cols=['rf_pred','medv']\n",
    "data[cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=101) \n",
    "regr.fit(X_train, y_train)\n",
    "mean_absolute_error(y_test, regr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variant of random forests, called extremely randomized trees (ExtraTrees), uses the same random subset\n",
    "of features method in selecting the best split at each branch in the tree. However, it also randomizes the\n",
    "discrimination threshold; where a decision tree normally chooses the most effective split between classes,\n",
    "ExtraTrees split at a random value.\n",
    "Due to the relatively efficient training of decision trees, a random forest algorithm can potentially support a\n",
    "large number of varied trees with the effectiveness of the classifier improving as the number of nodes\n",
    "increases. The randomness introduced provides a degree of robustness to noise or data change; like the\n",
    "bagging algorithms we reviewed earlier, however, this gain typically comes at the cost of a slight drop in\n",
    "performance. In the case of ExtraTrees, the robustness may increase further while the performance measure\n",
    "improves (typically a bias value reduces).\n",
    "The following code describes how ExtraTrees work in practice. As with our random subspace implementation,\n",
    "the code is very straightforward. In this case, we'll develop a set of models to compare how ExtraTrees shape\n",
    "up against tree and random forest approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

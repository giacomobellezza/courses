{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBoost algorithm has gained recently gained momentum and popularity in data-science\n",
    "competitions such as Kaggle (https://www.kaggle.com/ (https://www.kaggle.com/)) and the KDD-cup\n",
    "2015. As the authors (Tianqui Chen, Tong He, and Carlos Guestrin) report on papers they wrote on\n",
    "the algorithm, among 29 challenges held on Kaggle during 2015, 17 winning solutions used XGBoost\n",
    "as a standalone solution or as part of an ensemble of multiple different models.\n",
    "In their paper, XGBoost: A Scalable Tree Boosting System (which can be found at\n",
    "http://learningsys.org/papers/LearningSys_2015_paper_32.pdf\n",
    "(http://learningsys.org/papers/LearningSys_2015_paper_32.pdf)), the authors report that XGBoost\n",
    "was also used by every team that ended in the top 10 of the recent KDD-cup 2015.\n",
    "Apart from the successful performances in both accuracy and computational efficiency, XGBoost is\n",
    "also a scalable solution under different points of view. XGBoost represents a new generation of GBM\n",
    "algorithms thanks to important tweaks to the initial tree boost GBM algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sparse-aware algorithm; it can leverage sparse matrices, saving both memory (no\n",
    "need for dense matrices) and computation time (zero values are handled in a special\n",
    "way).\n",
    "\n",
    "Approximate tree learning (weighted quantile sketch), which bears similar results but in\n",
    "much less time than the classical complete explorations of possible branch cuts.\n",
    "\n",
    "Parallel computing on a single machine (using multi-threading in the phase of the search\n",
    "for the best split) and similarly distributed computations on multiple ones.\n",
    "\n",
    "Out-of-core computations on a single machine leveraging a data storage solution called\n",
    "Column Block. This arranges data on a disk by columns, thus saving time by pulling data\n",
    "from the disk as the optimization algorithm (which works on column vectors) expects it\n",
    "\n",
    "XGBoost can also deal with missing data in an effective way. Other tree ensembles\n",
    "based on standard decision trees require missing data first to be imputed using an offscale\n",
    "value, such as a negative number, in order to develop an appropriate branching of\n",
    "the tree to deal with missing values.\n",
    "\n",
    "An algorithm that accepts sparse data, which can leverage sparse matrices, saving both\n",
    "memory (no need for dense matrices) and computation time (zero values are handled in a\n",
    "special way)\n",
    "\n",
    "An approximate tree learning (weighted quantile sketch), which bears similar results but in\n",
    "much less time than the classical complete explorations of possible branch cuts\n",
    "\n",
    "Parallel computing on a single machine (using multithreading in the phase of the search for the\n",
    "best split) and similarly distributed computations on multiple ones\n",
    "\n",
    "Out-of-core computations on a single machine leveraging a data storage solution called\n",
    "Column Block, which arranges data on disk by columns, thus saving time by pulling data from\n",
    "disk as the optimization algorithm (which works on column vectors) expects it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost, instead, first fits all the non-missing values. After having created the branching for the\n",
    "variable, it decides which branch is better for the missing values to take in order to minimize the\n",
    "prediction error. Such an approach leads to both trees that are more compact and an effective\n",
    "imputation strategy leading to more predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost works much like AdaBoost with one key differenceâ€”the means by which the model is improved is\n",
    "different.\n",
    "At each iteration, XGBoost is seeking to improve the performance of the existing model set by reducing the\n",
    "residuals (the differences between targets and label predictions) of that ensemble. Every iteration, the model\n",
    "added is selected based on whether it is most able to reduce the existing ensemble's residuals. This is\n",
    "analogous to gradient descent (where a function is iteratively minimized by moving against a loss gradient);\n",
    "hence, the name Gradient Boosting.\n",
    "Gradient Boosting has proven to be highly successful in recent Kaggle contests, where it has supported the\n",
    "winners of the CrowdFlower Competition and Microsoft Malware Classification Challenge, along with many\n",
    "other structured data competitions in the final half of 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import pickle\n",
    "covertype_dataset = pickle.load(open(\"covertype_dataset.pickle\", \"rb\"))\n",
    "covertype_dataset.target = covertype_dataset.target.astype(int)\n",
    "covertype_X = covertype_dataset.data[:15000,:]\n",
    "covertype_y = covertype_dataset.target[:15000] -1\n",
    "covertype_val_X = covertype_dataset.data[15000:20000,:]\n",
    "covertype_val_y = covertype_dataset.target[15000:20000] -1\n",
    "covertype_test_X = covertype_dataset.data[20000:25000,:]\n",
    "covertype_test_y = covertype_dataset.target[20000:25000] -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "hypothesis = xgb.XGBClassifier(objective= \"multi:softprob\", max_depth = 24, gamma=0.1, subsample = 0.90, learning_rate=0.01, n_estimators = 500, nthread=-1)\n",
    "hypothesis.fit(covertype_X, covertype_y, eval_set=[(covertype_val_X, covertype_val_y)], eval_metric='merror', early_stopping_rounds=25, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xg_train = xgb.DMatrix( train_X, label=train_Y)\n",
    "xg_test = xgb.DMatrix(test_X, label=test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "print ('test accuracy:', accuracy_score(covertype_test_y, hypothesis.predict(covertype_test_X)))\n",
    "print (confusion_matrix(covertype_test_y, hypothesis.predict(covertype_test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "pd=fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "trainfile = urllib.URLopener()\n",
    "trainfile.retrieve(\"http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/poker.bz2\", \"pokertrain\n",
    ".bz2\")\n",
    "X,y = load_svmlight_file('pokertrain.bz2')\n",
    "dump_svmlight_file(X, y,'pokertrain', zero_based=True,query_id=None, multilabel=False)\n",
    "testfile = urllib.URLopener()\n",
    "testfile.retrieve(\"http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/poker.t.bz2\", \"pokertest\n",
    ".bz2\")\n",
    "X,y = load_svmlight_file('pokertest.bz2')\n",
    "dump_svmlight_file(X, y,'pokertest', zero_based=True,query_id=None, multilabel=False)\n",
    "del(X,y)\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix('/yourpath/pokertrain#dtrain.cache')\n",
    "dtest = xgb.DMatrix('/yourpath/pokertest#dtestin.cache')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

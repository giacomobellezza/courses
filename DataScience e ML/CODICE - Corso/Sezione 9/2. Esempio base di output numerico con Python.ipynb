{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vediamo un primo esempio di modello di regressione su numeri casuali "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.standard_normal(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = 2 * x * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a236ce828>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHwBK4v8BpN5EOhz2ldAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a23610588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = x[:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = x[81:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cl_train = y[:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cl_test = y[81:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 0s - loss: 378.3422 - val_loss: 357.2638\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s - loss: 357.2638 - val_loss: 337.3284\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s - loss: 337.3284 - val_loss: 320.1180\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s - loss: 320.1180 - val_loss: 302.0003\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s - loss: 302.0003 - val_loss: 285.1956\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s - loss: 285.1956 - val_loss: 268.0648\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s - loss: 268.0648 - val_loss: 251.3884\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s - loss: 251.3884 - val_loss: 235.2413\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s - loss: 235.2413 - val_loss: 219.6988\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s - loss: 219.6989 - val_loss: 204.8139\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s - loss: 204.8139 - val_loss: 190.6088\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s - loss: 190.6088 - val_loss: 177.0740\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s - loss: 177.0739 - val_loss: 164.1762\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s - loss: 164.1762 - val_loss: 151.8144\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s - loss: 151.8144 - val_loss: 140.0822\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s - loss: 140.0822 - val_loss: 128.9059\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s - loss: 128.9059 - val_loss: 118.2916\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s - loss: 118.2916 - val_loss: 108.2525\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s - loss: 108.2525 - val_loss: 98.7992\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s - loss: 98.7992 - val_loss: 89.9315\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s - loss: 89.9315 - val_loss: 81.5485\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s - loss: 81.5485 - val_loss: 73.8061\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s - loss: 73.8061 - val_loss: 66.5825\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s - loss: 66.5825 - val_loss: 59.8510\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s - loss: 59.8510 - val_loss: 53.5201\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s - loss: 53.5201 - val_loss: 47.7239\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s - loss: 47.7239 - val_loss: 42.3779\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s - loss: 42.3779 - val_loss: 37.4725\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s - loss: 37.4725 - val_loss: 32.9934\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s - loss: 32.9934 - val_loss: 28.9205\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s - loss: 28.9205 - val_loss: 25.2287\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s - loss: 25.2287 - val_loss: 21.8907\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s - loss: 21.8907 - val_loss: 18.8809\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s - loss: 18.8809 - val_loss: 16.1774\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s - loss: 16.1774 - val_loss: 13.7357\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s - loss: 13.7357 - val_loss: 11.5950\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s - loss: 11.5950 - val_loss: 9.7104\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s - loss: 9.7104 - val_loss: 8.0640\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s - loss: 8.0640 - val_loss: 6.6352\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s - loss: 6.6352 - val_loss: 5.4025\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s - loss: 5.4025 - val_loss: 4.3343\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s - loss: 4.3343 - val_loss: 3.4253\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s - loss: 3.4253 - val_loss: 2.6664\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s - loss: 2.6664 - val_loss: 2.0342\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s - loss: 2.0342 - val_loss: 1.5114\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s - loss: 1.5114 - val_loss: 1.0956\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s - loss: 1.0956 - val_loss: 0.7682\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s - loss: 0.7682 - val_loss: 0.5159\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s - loss: 0.5159 - val_loss: 0.3262\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s - loss: 0.3262 - val_loss: 0.1884\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 0s - loss: 0.1884 - val_loss: 0.0936\n",
      "Epoch 52/100\n",
      "80/80 [==============================] - 0s - loss: 0.0936 - val_loss: 0.0347\n",
      "Epoch 53/100\n",
      "80/80 [==============================] - 0s - loss: 0.0347 - val_loss: 0.0057\n",
      "Epoch 54/100\n",
      "80/80 [==============================] - 0s - loss: 0.0057 - val_loss: 6.9793e-04\n",
      "Epoch 55/100\n",
      "80/80 [==============================] - 0s - loss: 6.9793e-04 - val_loss: 0.0139\n",
      "Epoch 56/100\n",
      "80/80 [==============================] - 0s - loss: 0.0139 - val_loss: 0.0399\n",
      "Epoch 57/100\n",
      "80/80 [==============================] - 0s - loss: 0.0399 - val_loss: 0.0740\n",
      "Epoch 58/100\n",
      "80/80 [==============================] - 0s - loss: 0.0740 - val_loss: 0.1120\n",
      "Epoch 59/100\n",
      "80/80 [==============================] - 0s - loss: 0.1120 - val_loss: 0.1515\n",
      "Epoch 60/100\n",
      "80/80 [==============================] - 0s - loss: 0.1515 - val_loss: 0.1908\n",
      "Epoch 61/100\n",
      "80/80 [==============================] - 0s - loss: 0.1908 - val_loss: 0.2286\n",
      "Epoch 62/100\n",
      "80/80 [==============================] - 0s - loss: 0.2286 - val_loss: 0.2636\n",
      "Epoch 63/100\n",
      "80/80 [==============================] - 0s - loss: 0.2636 - val_loss: 0.2946\n",
      "Epoch 64/100\n",
      "80/80 [==============================] - 0s - loss: 0.2946 - val_loss: 0.3205\n",
      "Epoch 65/100\n",
      "80/80 [==============================] - 0s - loss: 0.3205 - val_loss: 0.3408\n",
      "Epoch 66/100\n",
      "80/80 [==============================] - 0s - loss: 0.3408 - val_loss: 0.3551\n",
      "Epoch 67/100\n",
      "80/80 [==============================] - 0s - loss: 0.3551 - val_loss: 0.3639\n",
      "Epoch 68/100\n",
      "80/80 [==============================] - 0s - loss: 0.3639 - val_loss: 0.3676\n",
      "Epoch 69/100\n",
      "80/80 [==============================] - 0s - loss: 0.3676 - val_loss: 0.3668\n",
      "Epoch 70/100\n",
      "80/80 [==============================] - 0s - loss: 0.3668 - val_loss: 0.3618\n",
      "Epoch 71/100\n",
      "80/80 [==============================] - 0s - loss: 0.3618 - val_loss: 0.3528\n",
      "Epoch 72/100\n",
      "80/80 [==============================] - 0s - loss: 0.3528 - val_loss: 0.3404\n",
      "Epoch 73/100\n",
      "80/80 [==============================] - 0s - loss: 0.3404 - val_loss: 0.3249\n",
      "Epoch 74/100\n",
      "80/80 [==============================] - 0s - loss: 0.3249 - val_loss: 0.3071\n",
      "Epoch 75/100\n",
      "80/80 [==============================] - 0s - loss: 0.3071 - val_loss: 0.2877\n",
      "Epoch 76/100\n",
      "80/80 [==============================] - 0s - loss: 0.2877 - val_loss: 0.2671\n",
      "Epoch 77/100\n",
      "80/80 [==============================] - 0s - loss: 0.2671 - val_loss: 0.2460\n",
      "Epoch 78/100\n",
      "80/80 [==============================] - 0s - loss: 0.2460 - val_loss: 0.2246\n",
      "Epoch 79/100\n",
      "80/80 [==============================] - 0s - loss: 0.2246 - val_loss: 0.2032\n",
      "Epoch 80/100\n",
      "80/80 [==============================] - 0s - loss: 0.2032 - val_loss: 0.1820\n",
      "Epoch 81/100\n",
      "80/80 [==============================] - 0s - loss: 0.1820 - val_loss: 0.1615\n",
      "Epoch 82/100\n",
      "80/80 [==============================] - 0s - loss: 0.1615 - val_loss: 0.1419\n",
      "Epoch 83/100\n",
      "80/80 [==============================] - 0s - loss: 0.1419 - val_loss: 0.1236\n",
      "Epoch 84/100\n",
      "80/80 [==============================] - 0s - loss: 0.1236 - val_loss: 0.1066\n",
      "Epoch 85/100\n",
      "80/80 [==============================] - 0s - loss: 0.1066 - val_loss: 0.0910\n",
      "Epoch 86/100\n",
      "80/80 [==============================] - 0s - loss: 0.0910 - val_loss: 0.0767\n",
      "Epoch 87/100\n",
      "80/80 [==============================] - 0s - loss: 0.0767 - val_loss: 0.0639\n",
      "Epoch 88/100\n",
      "80/80 [==============================] - 0s - loss: 0.0639 - val_loss: 0.0524\n",
      "Epoch 89/100\n",
      "80/80 [==============================] - 0s - loss: 0.0524 - val_loss: 0.0424\n",
      "Epoch 90/100\n",
      "80/80 [==============================] - 0s - loss: 0.0424 - val_loss: 0.0337\n",
      "Epoch 91/100\n",
      "80/80 [==============================] - 0s - loss: 0.0337 - val_loss: 0.0263\n",
      "Epoch 92/100\n",
      "80/80 [==============================] - 0s - loss: 0.0263 - val_loss: 0.0201\n",
      "Epoch 93/100\n",
      "80/80 [==============================] - 0s - loss: 0.0201 - val_loss: 0.0149\n",
      "Epoch 94/100\n",
      "80/80 [==============================] - 0s - loss: 0.0149 - val_loss: 0.0107\n",
      "Epoch 95/100\n",
      "80/80 [==============================] - 0s - loss: 0.0107 - val_loss: 0.0074\n",
      "Epoch 96/100\n",
      "80/80 [==============================] - 0s - loss: 0.0074 - val_loss: 0.0048\n",
      "Epoch 97/100\n",
      "80/80 [==============================] - 0s - loss: 0.0048 - val_loss: 0.0029\n",
      "Epoch 98/100\n",
      "80/80 [==============================] - 0s - loss: 0.0029 - val_loss: 0.0016\n",
      "Epoch 99/100\n",
      "80/80 [==============================] - 0s - loss: 0.0016 - val_loss: 7.0911e-04\n",
      "Epoch 100/100\n",
      "80/80 [==============================] - 0s - loss: 7.0911e-04 - val_loss: 2.2051e-04\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1, input_shape=(1,), activation=None))\n",
    "model.summary()\n",
    "adam = Adam(lr = 0.5)\n",
    "model.compile(loss='mse',\n",
    "              optimizer=adam)\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer=adam)\n",
    "\n",
    "history = model.fit(train, cl_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    validation_data=(train, cl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 20.01316261]], dtype=float32), array([-0.00469217], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00014263630146160722"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test, cl_test, verbose = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
